{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec8e18c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4000/4000 [==============================] - 100s 24ms/step - loss: 0.7605 - accuracy: 0.5862 - f1_metric: 0.5611 - auc: 0.8716 - val_loss: 0.5717 - val_accuracy: 0.6878 - val_f1_metric: 0.6877 - val_auc: 0.9223\n",
      "Epoch 2/10\n",
      "4000/4000 [==============================] - 98s 25ms/step - loss: 0.5821 - accuracy: 0.6803 - f1_metric: 0.6796 - auc: 0.9196 - val_loss: 0.5468 - val_accuracy: 0.7043 - val_f1_metric: 0.7042 - val_auc: 0.9296\n",
      "Epoch 3/10\n",
      "4000/4000 [==============================] - 158s 40ms/step - loss: 0.5627 - accuracy: 0.6930 - f1_metric: 0.6922 - auc: 0.9255 - val_loss: 0.5321 - val_accuracy: 0.7126 - val_f1_metric: 0.7126 - val_auc: 0.9334\n",
      "Epoch 4/10\n",
      "4000/4000 [==============================] - 96s 24ms/step - loss: 0.5527 - accuracy: 0.6997 - f1_metric: 0.6989 - auc: 0.9283 - val_loss: 0.5265 - val_accuracy: 0.7151 - val_f1_metric: 0.7145 - val_auc: 0.9347\n",
      "Epoch 5/10\n",
      "4000/4000 [==============================] - 99s 25ms/step - loss: 0.5477 - accuracy: 0.7030 - f1_metric: 0.7025 - auc: 0.9298 - val_loss: 0.5260 - val_accuracy: 0.7161 - val_f1_metric: 0.7160 - val_auc: 0.9350\n",
      "Epoch 6/10\n",
      "4000/4000 [==============================] - 99s 25ms/step - loss: 0.5448 - accuracy: 0.7065 - f1_metric: 0.7060 - auc: 0.9307 - val_loss: 0.5254 - val_accuracy: 0.7176 - val_f1_metric: 0.7175 - val_auc: 0.9355\n",
      "Epoch 7/10\n",
      "4000/4000 [==============================] - 101s 25ms/step - loss: 0.5420 - accuracy: 0.7080 - f1_metric: 0.7075 - auc: 0.9313 - val_loss: 0.5226 - val_accuracy: 0.7195 - val_f1_metric: 0.7191 - val_auc: 0.9361\n",
      "Epoch 8/10\n",
      "4000/4000 [==============================] - 102s 25ms/step - loss: 0.5403 - accuracy: 0.7099 - f1_metric: 0.7093 - auc: 0.9319 - val_loss: 0.5213 - val_accuracy: 0.7186 - val_f1_metric: 0.7186 - val_auc: 0.9360\n",
      "Epoch 9/10\n",
      "4000/4000 [==============================] - 99s 25ms/step - loss: 0.5373 - accuracy: 0.7112 - f1_metric: 0.7106 - auc: 0.9326 - val_loss: 0.5189 - val_accuracy: 0.7190 - val_f1_metric: 0.7188 - val_auc: 0.9367\n",
      "Epoch 10/10\n",
      "4000/4000 [==============================] - 102s 25ms/step - loss: 0.5365 - accuracy: 0.7126 - f1_metric: 0.7122 - auc: 0.9329 - val_loss: 0.5223 - val_accuracy: 0.7167 - val_f1_metric: 0.7166 - val_auc: 0.9359\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.5223 - accuracy: 0.7167 - f1_metric: 0.7166 - auc: 0.9359\n",
      "Test loss: 0.5222935676574707, Test accuracy: 0.7167325615882874, Test F1 Score: 0.7165713906288147, Test AUC: 0.935920774936676\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras.metrics import AUC\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "# Define column data types\n",
    "column_dtypes = {\n",
    "    'down': float, 'qtr': float, 'ydstogo': float, 'yardline_100': float, 'time': str, \n",
    "    'score_differential': float, 'no_score_prob': float, 'opp_fg_prob': float, \n",
    "    'opp_safety_prob': float, 'opp_td_prob': float, 'fg_prob': float, \n",
    "    'safety_prob': float, 'td_prob': float, 'play_type': str\n",
    "}\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"NFL Play by Play 2009-2018 (v5).csv\", dtype=column_dtypes, low_memory=False)\n",
    "\n",
    "# Convert 'time' column to 'time_elapsed'\n",
    "def convert_to_seconds(row):\n",
    "    if isinstance(row['time'], str):\n",
    "        time_parts = row['time'].split(':')\n",
    "        if len(time_parts) == 2:\n",
    "            minutes, seconds = map(int, time_parts)\n",
    "            time_in_current_qtr = minutes * 60 + seconds\n",
    "            elapsed_time_previous_qtrs = (row['qtr'] - 1) * 900\n",
    "            return elapsed_time_previous_qtrs + time_in_current_qtr\n",
    "    return np.nan\n",
    "\n",
    "data['time_elapsed'] = data.apply(convert_to_seconds, axis=1)\n",
    "data = data.dropna(subset=filtered_columns + ['time_elapsed'])\n",
    "\n",
    "# Filter and Encode 'play_type'\n",
    "data = data[data['play_type'].isin(['kickoff', 'extra_point', 'pass', 'run', 'punt', 'field_goal'])]\n",
    "le = LabelEncoder()\n",
    "data['PlayType_encoded'] = le.fit_transform(data['play_type'])\n",
    "\n",
    "# Define features and target\n",
    "filtered_columns = ['down', 'qtr', 'ydstogo', 'yardline_100', 'score_differential', \n",
    "                    'no_score_prob', 'opp_fg_prob', 'opp_safety_prob', 'opp_td_prob',\n",
    "                    'fg_prob', 'safety_prob', 'td_prob']\n",
    "features = filtered_columns + ['time_elapsed']\n",
    "X = data[features]\n",
    "y = data['PlayType_encoded']\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create sequences\n",
    "SEQUENCE_LENGTH = 5  # Increased sequence length for more context\n",
    "\n",
    "def create_sequences(data, sequence_length):\n",
    "    sequences = []\n",
    "    for index in range(len(data) - sequence_length + 1):\n",
    "        sequences.append(data[index: index + sequence_length])\n",
    "    return np.array(sequences)\n",
    "\n",
    "X_train_sequences = create_sequences(X_train_scaled, SEQUENCE_LENGTH)\n",
    "X_test_sequences = create_sequences(X_test_scaled, SEQUENCE_LENGTH)\n",
    "y_train_cat = to_categorical(y_train[SEQUENCE_LENGTH - 1:])\n",
    "y_test_cat = to_categorical(y_test[SEQUENCE_LENGTH - 1:])\n",
    "\n",
    "# Custom F1 Score metric\n",
    "def f1_metric(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "# Define the LSTM model with Batch Normalization\n",
    "model = Sequential([\n",
    "    LSTM(100, input_shape=(SEQUENCE_LENGTH, len(features)), return_sequences=True),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    LSTM(100, return_sequences=True),  # Additional LSTM layer\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    LSTM(100),  # Additional LSTM layer\n",
    "    BatchNormalization(),\n",
    "    Dense(64, activation='relu'),  # Additional dense layer\n",
    "    Dropout(0.5),\n",
    "    Dense(y_train_cat.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# Optimizer with reduced learning rate\n",
    "optimizer = Adam(learning_rate=0.0005)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', f1_metric, AUC(name='auc')])\n",
    "\n",
    "# Train with increased epochs\n",
    "model.fit(X_train_sequences, y_train_cat, epochs=10, batch_size=64,\n",
    "          validation_data=(X_test_sequences, y_test_cat), callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "# Evaluate\n",
    "score = model.evaluate(X_test_sequences, y_test_cat, verbose=1)\n",
    "print(f\"Test loss: {score[0]}, Test accuracy: {score[1]}, Test F1 Score: {score[2]}, Test AUC: {score[3]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcb098e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
